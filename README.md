# Shannons Scaling Laws
A creative exploration into Shannon’s Entropy and how entropy, meaning, and language shifts across contexts—and how this translates to scaling laws. For example: Could models trained in Mandarin have a scaling laws advantage in information density per token?
