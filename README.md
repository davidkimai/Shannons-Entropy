# Shannons Scaling Laws
A creative exploration into Shannon’s Entropy and how entropy, meaning, and language shifts across contexts—and how this translates to scaling laws. For example: Could models trained in Mandarin have a scaling laws advantage in information density per token?

## Papers Contributing to this exploration: 
## [Vaswani et al. (2017). "Attention Is All You Need."](https://arxiv.org/abs/1706.03762)
## [Shannon, C. E. (1951). "Prediction and Entropy of Printed English."](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf)
## [Kaplan, J., et al. (2020). "Scaling Laws for Neural Language Models."](https://arxiv.org/abs/2001.08361)
## [Hoffmann, J., et al. (2022). "Training Compute-Optimal Large Language Models."](https://arxiv.org/abs/2203.15556)
## [Gildea, D., & Jaeger, T. F. (2015). "Human languages order information efficiently."](https://arxiv.org/abs/1510.02823)
## [Coupé, C., et al. (2019). "Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche." ](https://pubmed.ncbi.nlm.nih.gov/32047854/)
## [Frost, R. (2012). "Towards a Universal Model of Reading."](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/towards-a-universal-model-of-reading/215B8EC1ABA1987401C440455F24C765)
