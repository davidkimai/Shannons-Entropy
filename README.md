# Shannons-Entropy
A creative exploration into Shannonâ€™s Entropy and how entropy, meaning, and language shifts across contexts and how this translates to scaling laws. For example: Could models trained in Mandarin have a scaling laws advantage in information density per token?
