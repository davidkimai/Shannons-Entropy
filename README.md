# Shannons Scaling Laws
A creative exploration into Shannon’s Entropy and how entropy, meaning, and language shifts across contexts—and how this translates to scaling laws. For example: Could models trained in Mandarin have a scaling laws advantage in information density per token?

> This repo will be updated as new discoveries are made

# Join Me In This Fascinating Exploration!

## [Papers Guiding This Contribution](https://arc.net/folder/E9A1BCB2-4F59-40C6-9730-18127D63D9E4)

![image](https://github.com/user-attachments/assets/957c91f0-839b-46b9-88a2-cffc41841c08)


### [Vaswani et al. (2017). "Attention Is All You Need."](https://arxiv.org/abs/1706.03762)

### [Shannon, C. E. (1948). A Mathematical Theory of Communication. Bell System Technical Journal, 27(3), 379–423.](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf)

### [Shannon, C. E. (1951). "Prediction and Entropy of Printed English."](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf)
### [Kaplan, J., et al. (2020). "Scaling Laws for Neural Language Models."](https://arxiv.org/abs/2001.08361)

### [Hernandez, D., et al. (2022), "Scaling Laws and Interpretability of Learning from Repeated Data"](https://arxiv.org/abs/2205.10487)
### [Elhage, N., et al. (2021), "A Mathematical Framework for Transformer Circuits"](https://transformer-circuits.pub/2021/framework/index.html)
### [Hoffmann, J., et al. (2022). "Training Compute-Optimal Large Language Models."](https://arxiv.org/abs/2203.15556)
### [Bentz, C., et al (2017). "The Entropy of Words—Learnability and Expressivity across More than 1000 Languages"](https://www.mdpi.com/1099-4300/19/6/275)

### [Gildea, D., & Jaeger, T. F. (2015). "Human languages order information efficiently."](https://arxiv.org/abs/1510.02823)
### [Coupé, C., et al. (2019). "Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche." ](https://pubmed.ncbi.nlm.nih.gov/32047854/)
### [Frost, R. (2012). "Towards a Universal Model of Reading."](https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/towards-a-universal-model-of-reading/215B8EC1ABA1987401C440455F24C765)
### [Peirce, C. S. (1931-58). "Collected Papers."](https://colorysemiotica.wordpress.com/wp-content/uploads/2014/08/peirce-collectedpapers.pdf)
### [Tishby, N., et al. (2015). "Deep Learning and the Information Bottleneck Principle."](https://arxiv.org/abs/1503.02406)


